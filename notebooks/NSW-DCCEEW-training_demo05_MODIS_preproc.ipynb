{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>NSW DCCEEW training â€“ Demo 5: MODIS time series pre-processing</u>\n",
    "\n",
    " - <b>Author</b>: Eric.Lehmann@csiro.au\n",
    " - <b>Release date / version</b>: Aug. 2024, v1.0\n",
    " - <b>Dev. platform</b>: CSIRO ADIAS/ADS (hub.adias.aquawatchaus.space)\n",
    " - <b>Server profile</b>: EASI Open Data Cube No ML &ndash; Version 2023.10.2 \n",
    " - <b>Server resources</b>: 32 CPU &ndash; 64GB RAM\n",
    " - <b>Python kernel</b>: `Python 3 (ipykernel)`\n",
    " - <b>Dask</b>: Local cluster\n",
    " \n",
    "# Overview\n",
    "\n",
    "This notebook loads up a time series of MODIS data and subsequently applies empirical water quality (WQ) for TSS and Chl-a algorithms to this dataset.\n",
    "\n",
    "The water quality (TSS, Chl-a) values are then extracted and summarised (averaged) over a small patch of water in front of the selected river mouth. The resulting time series can subsequently written to .csv file.\n",
    "\n",
    "The MODIS data is loaded from ADIAS: Aqua &ndash; https://explorer.adias.aquawatchaus.space/products/nasa_aqua_l2_oc, and Terra &ndash; https://explorer.adias.aquawatchaus.space/products/nasa_terra_l2_oc.\n",
    "\n",
    "# User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selected river systems: lat/lon locations, names, and sub-catchment names\n",
    "riv_loc = (151.34428, -33.5677)   # selected location -- river mouth, Haweksbury\n",
    "csv_file = None   # or output .csv file name to write results to, e.g. \"MODIS_data.csv\"\n",
    "\n",
    "latlon_buf = 0.5   # buffer around selected lat/lon location for data load (for visualisation purpose)\n",
    "# date_range = ('2002-01-01','2025-01-01')   # selected time window ... FULL TIME SERIES\n",
    "date_range = ('2020-01-01','2025-01-01')   # selected time window ... FULL TIME SERIES\n",
    "\n",
    "# Number of buffer pixels (MODIS resolution) for various masks\n",
    "n_pix_dilation = 10   # to select a patch of water around the selected river mouth locations\n",
    "n_pix_coast_buf = 3   # buffer around the coastline to be removed from above patch\n",
    "\n",
    "WQparam = 'TSS'   # selected WQ parameter for demo plots\n",
    "\n",
    "n_workers = None    # for local Dask cluster\n",
    "# n_workers = 12; workers_mem = 8    # for Dask Gateway cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System\n",
    "import sys, os\n",
    "import pickle, json\n",
    "import itertools\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "### Data handling\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "### Dask\n",
    "import dask\n",
    "from dask.distributed import wait\n",
    "import dask.array as da\n",
    "\n",
    "### Display / plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "### Data cube\n",
    "import datacube\n",
    "# from datacube.utils import masking\n",
    "# from odc.algo import enum_to_bool\n",
    "dc = datacube.Datacube(app=\"AW_AI4M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here making use of a couple of functions from the base EASI notebooks &ndash; these can be accessed by `git clone`-ing the following repo: `https://github.com/csiro-easi/easi-notebooks.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Misc\n",
    "sys.path.append('/home/jovyan/git_hub_notebooks/scripts/')   # standard EASI github repo, cloned from: https://dev.azure.com/csiro-easi/easi-hub-public/_git/hub-notebooks\n",
    "from app_utils import display_map   # to display the region of interest\n",
    "import notebook_utils   # for xarray_object_size()\n",
    "\n",
    "# Eric's own function for getting coastline data:\n",
    "exec(open(\"./get_coastline.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Filter out following warnings:\n",
    "# /env/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 32.04 MiB.\n",
    "# This may cause some slowdown. Consider scattering data ahead of time and using futures.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = LinearSegmentedColormap.from_list(\"cmp\", [\"gainsboro\", \"gainsboro\"])   # \"dummy\" colormap for greyed out land pixels\n",
    "\n",
    "assert WQparam=='TSS', 'Need to define variables for other WQ parameters...'\n",
    "WQunits = 'mg/L'         # WQ units\n",
    "cmap = 'jet'   # if lower_is_better, otherwise: cmap = 'jet_r'\n",
    "norm = colors.LogNorm()   # if wq_log_scale, otherwise: norm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_longitude = riv_loc[0]-latlon_buf\n",
    "min_latitude = riv_loc[1]-latlon_buf\n",
    "max_longitude = riv_loc[0]+latlon_buf\n",
    "max_latitude = riv_loc[1]+latlon_buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "Flexible open-source library for parallel and distributed computing in Python. It provides integration with various Python libraries like NumPy, Pandas, and scikit-learn to enable parallel execution across multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Local Dask cluster\n",
    "if n_workers is None:  # local Dask cluster using all available CPUs\n",
    "    \n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    # sys.path.append('/home/jovyan/git_hub_notebooks/scripts/')\n",
    "    # import notebook_utils   # for localcluster_dashboard()\n",
    "\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(f\"Local cluster dashboard: {notebook_utils.localcluster_dashboard(client,server='https://hub.adias.aquawatchaus.space')}\")\n",
    "    display(cluster)\n",
    "\n",
    "### Use the following to shut down this cluster:\n",
    "# cluster.close()\n",
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Dask Gateway cluster @ n_workers\n",
    "if n_workers is not None:\n",
    "\n",
    "    from dask_gateway import Gateway\n",
    "    gateway = Gateway()\n",
    "    \n",
    "    # shutdown_all_clusters...\n",
    "    clusterRpts = gateway.list_clusters()\n",
    "    if len(clusterRpts)>0: print(f\"Shutting down running clusters:\\n {clusterRpts}\")\n",
    "    for cluster in clusterRpts:\n",
    "        c = gateway.connect(cluster.name)\n",
    "        c.shutdown()\n",
    "\n",
    "    print(\"Creating new Gateway cluster...\")\n",
    "\n",
    "    options = gateway.cluster_options()\n",
    "    options.node_selection = \"worker\" \n",
    "    options.worker_cores = 8\n",
    "    options.worker_memory = workers_mem\n",
    "\n",
    "    cluster = gateway.new_cluster(cluster_options=options)\n",
    "    cluster.scale(n_workers)\n",
    "    display( cluster )\n",
    "\n",
    "    ### Wait for all workers to start\n",
    "    client = cluster.get_client()\n",
    "    display( client )\n",
    "    client.sync( client._wait_for_workers, n_workers=n_workers )\n",
    "\n",
    "### Use the following to shut down this cluster:\n",
    "# cluster.shutdown()\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_map(x=(min_longitude,max_longitude), y=(min_latitude,max_latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read and pre-process MODIS data\n",
    "\n",
    "Load and apply standard pre-procsesing steps to the MODIS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_bands = ['rrs_443', 'rrs_469', 'rrs_488', 'rrs_547', 'rrs_555', 'l2_flags']\n",
    "\n",
    "query = { 'product': 'nasa_aqua_l2_oc',                     # MODIS Aqua product\n",
    "          'measurements': load_bands,\n",
    "          'x': (min_longitude, max_longitude),    # \"x\" axis bounds\n",
    "          'y': (min_latitude, max_latitude),      # \"y\" axis bounds\n",
    "          'time': date_range,                     # Any parsable date strings\n",
    "          'output_crs': 'epsg:4326',              # EPSG code\n",
    "          # 'resolution': (0.01, 0.01),           # Target resolution\n",
    "          # 'group_by': 'solar_day',                # Scene ordering  ---  potentially messes up L2 flags with multiple daily observations...!?\n",
    "          'dask_chunks': {'latitude': -1, 'longitude': -1, 'time': 1} }  # Dask chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Load MODIS Aqua dataset\n",
    "data_aqua = dc.load(**query).persist()\n",
    "display( notebook_utils.xarray_object_size(data_aqua) )\n",
    "display(data_aqua)\n",
    "_ = wait(data_aqua)   # actual data loading happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Load MODIS Terra dataset\n",
    "query['product'] = 'nasa_terra_l2_oc'   # MODIS Terra product\n",
    "data_terra = dc.load(**query).persist()\n",
    "display( notebook_utils.xarray_object_size(data_terra) )\n",
    "display(data_terra)\n",
    "_ = wait(data_terra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(data_aqua.sizes)!=0 and len(data_terra.sizes)!=0, 'One of Aqua and Terra time series is empty.'   # need to update code in this case...\n",
    "assert data_terra.latitude.identical(data_aqua.latitude), \"Different latitude coords\"\n",
    "assert data_terra.longitude.identical(data_aqua.longitude), \"Different longitude coords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Combine the Aqua and Terra time series\n",
    "data = xr.concat([data_aqua, data_terra], dim='time').sortby('time').persist()   # alternatively: xr.merge([data_sst_aqua, data_sst_terra])\n",
    "display( notebook_utils.xarray_object_size(data) )\n",
    "display(data)\n",
    "_ = wait(data)\n",
    "\n",
    "assert np.all(np.diff(data.time.values).astype(float)>0), \"Some time slices are out of order.\"   # ensure the data is ordered properly temporally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tchks = 256\n",
    "data = data.chunk(time=tchks).persist()   # re-chunk to sensible chunk sizes (small spatial footprint)\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Filter out (int16) 'nodata' pixels --> NaN (float32)\n",
    "rrs_vars = [bb for bb in list(data.variables) if bb.lower().startswith('rrs')]   # all 'rrs' vars\n",
    "for vv in rrs_vars:\n",
    "    nodat = data[vv].attrs['nodata']\n",
    "    data[vv] = data[vv].where(data[vv]!=nodat)\n",
    "\n",
    "data = data.persist()\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Filter data as per the data's L2 flags layer\n",
    "bad_pixel_flags = ( 'ATMFAIL', 'LAND', 'HIGLINT', 'HTLT', 'HISATZEN', 'STRAYLIGHT', 'CLDICE', 'COCCOLITH',     # PQ-affected pixel categories to filter out. Not used: CHLFAIL, CHLWARN\n",
    "                    'HISOLZEN', 'LOWLW', 'NAVWARN', 'MAXAERITER', 'ATMWARN', 'SEAICE', 'NAVFAIL', 'nodata' )   # L3 Mask Default from: https://oceancolor.gsfc.nasa.gov/resources/atbd/ocl2flags/\n",
    "\n",
    "fdict = data.l2_flags.attrs['flags_definition']['l2_flags']['values']   # flags dict: '1': 'ATMFAIL', '2': 'LAND', '4': ... etc.\n",
    "fdict[ str(data.l2_flags.attrs['nodata']) ] = 'nodata'   # also remove L2_flags 'nodata' pixels here\n",
    "\n",
    "init = True\n",
    "for kk in fdict:\n",
    "    if fdict[kk] in bad_pixel_flags:\n",
    "        msk = ( da.bitwise_and(data.l2_flags,int(kk)) == int(kk) )\n",
    "        if init: init = False; bad_pixel_mask = msk\n",
    "        else: bad_pixel_mask = bad_pixel_mask | msk\n",
    "\n",
    "good_pixel_mask = (~bad_pixel_mask).persist()\n",
    "_ = wait(good_pixel_mask)\n",
    "\n",
    "### Apply good pixel mask\n",
    "data = data.where(good_pixel_mask).persist()\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Visualise some outputs\n",
    "plt_ind = np.linspace(1, data.sizes['time'], 10, dtype='int') - 1   # some selected time slices to display\n",
    "good_pixel_mask[plt_ind].plot(col='time', col_wrap=5, figsize=(16,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Apply MODIS band-specific scaling\n",
    "for vv in rrs_vars:\n",
    "    add_offset = data[vv].attrs['add_offset']\n",
    "    scale_fac = data[vv].attrs['scale_factor']\n",
    "    data[vv] = data[vv] * scale_fac + add_offset\n",
    "\n",
    "data = data.persist()\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create mask of positive Rrs values (mask out pixels with negative Rrs values in any band)\n",
    "for ii,vv in enumerate(rrs_vars):\n",
    "    if ii==0: rrs_mask = ( data[vv]>=0 )\n",
    "    else: rrs_mask = rrs_mask & (data[vv]>=0)   #np.logical_and(rrs_mask, data[vv]>=0)\n",
    "\n",
    "rrs_mask = rrs_mask.persist()\n",
    "_ = wait(rrs_mask)\n",
    "\n",
    "### Apply negative-Rrs mask\n",
    "data = data.where(rrs_mask).persist()\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Drop empty time slices from the dataset\n",
    "data = data.dropna('time',how='all')\n",
    "_ = wait(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Some plots to visually check the WQ results\n",
    "plt_ind = np.linspace(1, data.sizes['time'], 15, dtype='int') - 1   # some selected time slices to display\n",
    "pp = data.rrs_488[plt_ind].plot( col='time', col_wrap=5, robust=True, norm=norm, cmap=cmap, \n",
    "                                  figsize=(20,10), cbar_kwargs={'label':f'{WQparam} [{WQunits}]'} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have a fully pre-processed dataset of MODIS data (Rrs values i.e. remote sensing reflectance).\n",
    "\n",
    "# Calculate WQ parameters\n",
    "\n",
    "Next, we apply a further (custom) scaling factor that will allow us to convert these `Rrs` values to water quality parameters &ndash; note that this next scaling is not something that needs to be applied to the data as a standard MODIS pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Custom Rrs-specific band scaling ('band values to single wavelength' transform)\n",
    "scale_fac = 1.2209 \n",
    "exp_fac = 1.0359\n",
    "\n",
    "for vv in rrs_vars:\n",
    "    data[vv] = scale_fac * pow(data[vv], exp_fac)\n",
    "\n",
    "data = data.persist()\n",
    "_ = wait(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MODIS-based WQ parameters calculated below are from preliminary empirical CSIRO algorithms and should be used for demo / illustration purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Now we can calculate the MODIS-based WQ parameters of interest\n",
    "band_ratio = data['rrs_469'] / data['rrs_555']   # data['rrs_466'] / data['rrs_554']   # Xarray DataArray\n",
    "data[\"TSS\"] = (1.3797*pow(band_ratio, -0.938))\n",
    "\n",
    "# fmax: element-wise maximum of array elements -- NaNs are ignored when possible: if one of the elements being compared is NaN, then the non-nan element is returned\n",
    "band_ratio = np.fmax(data['rrs_443'], data['rrs_488']) / data['rrs_547']   # np.fmax(data['rrs_442'], data['rrs_488']) / data['rrs_547']\n",
    "data[\"Tchl-a\"] = (1.2512*pow(band_ratio, -1.694))\n",
    "\n",
    "data = data.persist()\n",
    "_ = wait(data)   #progress(data, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Remove the Rrs bands -- not needed any longer\n",
    "data = data.drop_vars(load_bands).persist()\n",
    "data = data.dropna('time',how='all').chunk(time=tchks).persist()\n",
    "_ = wait(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Some quick plots to double-check raw data\n",
    "plt_ind = np.linspace(1, data.sizes['time'], 10, dtype='int') - 1   # some selected time slices to display\n",
    "data.TSS[plt_ind].plot(col='time', col_wrap=5, figsize=(16,6), robust=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual checks of WQ parameter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Generate coastline and land mask for current extents, in projected CRS\n",
    "land_mask, shp_poly = get_coastline( ds_lon_vec=data.longitude.values, ds_lat_vec=data.latitude.values)   #, do_plot=True)#, buf=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check time gaps between observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffvec = np.diff( data.time ) / np.timedelta64(1, 'D')\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data.time.values[1:], diffvec, 'o', markersize=1)\n",
    "plt.axhline(y=1.0, linestyle=':', color='red')\n",
    "plt.ylabel('days'); plt.title('Nr of days between consecutive time slices, vs. time');\n",
    "\n",
    "### Overview of main time gaps in the time series...\n",
    "np.unique(np.round(diffvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values above `1.0` indicate missing time slices (missing data due to filtered-out pixels). Values below `1.0` **might** indicate multiple time slices per day (or small gaps across midnight)...\n",
    "\n",
    "## Group all time slices acquired over same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Group by day\n",
    "tmp = data.time.resample(time='1D')\n",
    "tmp = tmp.count('time').values   # nr of time slices in each date/group along TS (even where empty / none --> 'nan' count value)\n",
    "if np.any( ~np.isnan(tmp) & (tmp!=1.) ):   # some time slices less than 1 day apart, i.e. more than 1 time slices per day\n",
    "    data = data.resample(time='1D', skipna=True).mean('time').persist()   # skipna=True --> still creates empty time slices (one for EACH day along TS)\n",
    "    data = data.dropna('time',how='all').persist()   # drop them!\n",
    "    display(data)\n",
    "    _ = wait(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffvec = np.diff( data.time ) / np.timedelta64(1, 'D')\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data.time.values[1:], diffvec, 'o', markersize=1)\n",
    "plt.axhline(y=1.0, linestyle=':', color='red')\n",
    "plt.ylabel('days'); plt.title('Nr of days between consecutive time slices, vs. time');\n",
    "\n",
    "### Overview of main time gaps in the time series...\n",
    "np.unique(np.round(diffvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Here using the overall mean and coefficient of variation, for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate overall mean and CV\n",
    "res = data.mean('time')\n",
    "res = res.where(land_mask).persist()\n",
    "_ = wait(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = data.std('time') / res\n",
    "res2 = res2.where(land_mask)\n",
    "res2 = res2.rename_vars({'TSS':'TSS_CV', 'Tchl-a':'Tchl-a_CV'}).persist()\n",
    "_ = wait(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['TSS_CV'] = res2.TSS_CV\n",
    "res['Tchl-a_CV'] = res2['Tchl-a_CV']\n",
    "res = res.persist()\n",
    "_ = wait(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plots of results, visual checks\n",
    "axratio = res.sizes['latitude'] / res.sizes['longitude']   # plot scaling for nice(r) layout\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(23,23*axratio/2.6))   # 2.6 = 2 horizontal plots + some buffer\n",
    "\n",
    "res[f'{WQparam}'].plot(robust=True, cmap=cmap, cbar_kwargs={'label':f'{WQparam} [{WQunits}]'}, ax=ax1)   # norm=norm, \n",
    "ax1.set_title(f\"Mean of {WQparam}\")\n",
    "ax1.set_aspect('equal','box')\n",
    "land_mask.plot(ax=ax1, add_colorbar=False, add_labels=False, cmap=cmp)\n",
    "shp_poly.boundary.plot(ax=ax1, color='black', linewidth=1);\n",
    "\n",
    "(res[f'{WQparam}_CV']*100.).plot(robust=True, cmap=\"jet\", cbar_kwargs={'label':'C.V. [%]'}, ax=ax2)\n",
    "ax2.set_title(f\"C.V. of {WQparam}\")\n",
    "ax2.set_aspect('equal','box')\n",
    "land_mask.plot(ax=ax2, add_colorbar=False, add_labels=False, cmap=cmp)\n",
    "shp_poly.boundary.plot(ax=ax2, color='black', linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the above plots can provide insight into whether a robust approach is needed for the mean and C.V. calculations.\n",
    "\n",
    "# Create mask of pixels in front of river mouth\n",
    "\n",
    "The mask of selected WQ pixels is created from the intersection of:\n",
    "  - a circle of radius `n_pix_dilation` pixels (Landsat resolution) centred over the selected river mouth location, and\n",
    "  - the ocean mask (derived from the coastline shape file), eroded by `n_pix_coast_buf` pixels (Landsat resolution) to remove potential artefacts close to the shoreline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Binary image dilation function\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def expand_mask(mask_arr, npix, expand_true=True):\n",
    "    # Uses the True/False (masked/non-masked) values in the array 'mask_arr' and \n",
    "    # expands the True values spatially by 'npix' pixels. The value 'npix' can be\n",
    "    # non-integer, i.e. the mask can be expanded by any spatial distance.\n",
    "    # Originally expands True values (when expand_true=True). Can expand False \n",
    "    # values by setting expand_true=False.\n",
    "    nmid = np.floor(npix)\n",
    "    nmax = int( nmid*2 + 1 )\n",
    "    struc = np.zeros((nmax, nmax), dtype='bool')\n",
    "    for ii in range(nmax):   # create desired binary structure for morphological operation\n",
    "        for jj in range(ii,nmax):\n",
    "            if pdist( [[nmid,nmid], [ii,jj]] ) <= npix:\n",
    "                struc[ii,jj] = True\n",
    "                struc[jj,ii] = True\n",
    "    if expand_true:\n",
    "        return ndimage.binary_dilation(mask_arr, structure=struc)\n",
    "    else:\n",
    "        return ~ndimage.binary_dilation(~mask_arr.astype(bool), structure=struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ROI mask & example of resulting TSS map (overall mean)\n",
    "\n",
    "mask = xr.full_like(data.TSS[0],0.0).compute().copy()\n",
    "\n",
    "### Initialise seed pixel at river loc\n",
    "xind = np.argmin(abs(mask.longitude-riv_loc[0]).values)\n",
    "yind = np.argmin(abs(mask.latitude-riv_loc[1]).values)\n",
    "mask[yind,xind] = 1.0\n",
    "\n",
    "### Expand mask by selected nr of pixels ... somewhat time consuming\n",
    "mask2 = expand_mask(mask.values, n_pix_dilation, expand_true=True)\n",
    "mask.data = mask2\n",
    "\n",
    "### Create raster mask from coastline polygon\n",
    "mask2 = rasterio.features.rasterize( ((feat['geometry'], 1) for feat in shp_poly.iterfeatures()),\n",
    "                                     out_shape = (mask.sizes['latitude'],mask.sizes['longitude']),\n",
    "                                     transform = data.affine )\n",
    "mask2 = expand_mask(mask2, n_pix_coast_buf, expand_true=True)\n",
    "mask2 = xr.DataArray(mask2, coords=(mask.latitude, mask.longitude))\n",
    "\n",
    "### Combine river mouth area mask and coast mask\n",
    "mask = mask.where(~mask2.astype('bool'), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Mask WQ time series data\n",
    "data_msk = data.where(mask)\n",
    "data_msk = data_msk.dropna('time',how='all').persist()\n",
    "_ = wait(data_msk)\n",
    "data_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plots\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(18,3.8))\n",
    "\n",
    "mask.plot(ax=ax1, cbar_kwargs={'label':'mask'})\n",
    "shp_poly.boundary.plot(ax=ax1, color='grey')\n",
    "ax1.set_title(f\"River mouth mask\")\n",
    "ax1.set_aspect('equal','box')\n",
    "\n",
    "### Example plot for selected param\n",
    "data_msk[WQparam].mean('time').plot(robust=True, cmap=cmap, cbar_kwargs={'label':f'{WQparam} [{WQunits}]'}, ax=ax2)   #, norm=norm)\n",
    "shp_poly.boundary.plot(ax=ax2, color='grey')\n",
    "ax2.set_title(f\"Overall mean of {WQparam}\")\n",
    "ax2.set_aspect('equal','box')\n",
    "\n",
    "(res[f'{WQparam}_CV'].where(mask)*100.).plot(robust=True, cmap=\"jet\", cbar_kwargs={'label':f'{WQparam} C.V. [%]'}, ax=ax3)\n",
    "ax3.set_title(f\"C.V. of {WQparam}\")\n",
    "ax3.set_aspect('equal','box')\n",
    "shp_poly.boundary.plot(ax=ax3, color='black', linewidth=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffvec = np.diff( data_msk.time ) / np.timedelta64(1, 'D')\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data_msk.time.values[1:], diffvec, 'o', markersize=1)\n",
    "plt.ylabel('days'); plt.title('Nr of days between consecutive observations, vs. time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial statistics at river mouth\n",
    "\n",
    "Here we derive the statistics of interest over all valid pixels spatially (within AOI). We are here not taking the mean and standard deviation due to the potentially \"skewed\" nature of the distribution of WQ (TSS) values &ndash; large standard deviations can potentially lead to very small, and even negative WQ values corresponding to the bottom of the standard error range (mean minus standard deviation). Such negative values would be physically meaningless, while very small values would lead to issues when plotting the results in a logarithmic scale.\n",
    "\n",
    "Instead of the mean & st. dev., we here calculate the median (50-th percentile) and inter-quartile range (25-th and 75-th percentiles) of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qntle_fcn(ds, qnt):   # function applied to each Rrs band (DataArray)\n",
    "    tmp = ds.chunk(dict(time=-1)).quantile(q=qnt, dim=['longitude','latitude'])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WQlow_quart = data_msk.map(qntle_fcn,qnt=0.25).persist()\n",
    "WQupp_quart = data_msk.map(qntle_fcn,qnt=0.75).persist()\n",
    "WQmed_quart = data_msk.map(qntle_fcn,qnt=0.5).persist()\n",
    "WQmin_quart = data_msk.min(['longitude','latitude']).persist()\n",
    "WQmax_quart = data_msk.max(['longitude','latitude']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Plots\n",
    "for param in ['TSS','Tchl-a']:\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.gca().plot(WQlow_quart.time, WQlow_quart[param],color='lightgrey', label='IQR')\n",
    "    plt.gca().plot(WQupp_quart.time, WQupp_quart[param],color='lightgrey')\n",
    "    plt.gca().plot(WQmin_quart.time, WQmin_quart[param],color='lightgreen', ls=':', label='min/max')\n",
    "    plt.gca().plot(WQmax_quart.time, WQmax_quart[param],color='lightgreen', ls=':')\n",
    "    plt.plot(WQmed_quart.time, WQmed_quart[param], label='median')\n",
    "    plt.gca().set_yscale('log'); plt.gca().legend()\n",
    "    plt.gca().set_xlabel('time'); plt.gca().set_ylabel(f'{param} at river mouth')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving average WQ data to .csv file\n",
    "\n",
    "Calculate the average TSS and Tchl-a at the river mouth and save the time series to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_msk_mean = data_msk.mean(['longitude','latitude']).persist()\n",
    "data_msk_mean = data_msk_mean.dropna('time',how='all').persist()\n",
    "data_msk_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = { 'date': data_msk_mean.time.dt.date.values, \n",
    "         'TSS_mg/litre': data_msk_mean.TSS.values,\n",
    "         'Tchl-a_ug/litre': data_msk_mean['Tchl-a'].values }\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "if csv_file is not None: \n",
    "    df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_workers is None:  # local Dask cluster\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "else:   # Gateway cluster\n",
    "    cluster.shutdown()\n",
    "    client.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### End notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe717451bb7ecc5471f08b246bd7dc7e2d4d0aede347a8d2c38bb7acb6040425"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
